{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d67a584-afe4-4972-a3cc-f257f5b3e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinRagAssist - Smart Investment Advisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1d4928-a3f6-4023-a995-094982c083d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 1 — Imports, config, load_data\n",
    "\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Paths\n",
    "DATA_CSV = r\"C:\\Users\\rahil\\Downloads\\cleaned_data_final.csv\"\n",
    "XGB_MODEL_PATH = r\"C:\\Users\\rahil\\Downloads\\xgb_risk_model.joblib\"\n",
    "\n",
    "# Final feature set (training uses rich features, UI uses grouped features)\n",
    "FEATURE_COLS = [\n",
    "    # Rich numeric features from CSV\n",
    "    \"Age\",\n",
    "    \"Income Level\",\n",
    "    \"Account Balance\",\n",
    "    \"Deposits\",\n",
    "    \"Withdrawals\",\n",
    "    \"Transfers\",\n",
    "    \"International Transfers\",\n",
    "    \"Investments\",\n",
    "    \"Loan Amount\",\n",
    "    \"Loan Term (Months)\",\n",
    "    \"Net Savings\",\n",
    "    \"Loan to Income Ratio\",\n",
    "    \"Investment Ratio\",\n",
    "\n",
    "    # Simplified features used in UI\n",
    "    \"AgeGroup\",\n",
    "    \"IncomeGroup\",\n",
    "    \"EmploymentStatus\",\n",
    "    \"LoanStatus\",\n",
    "    \"InvestmentGoal\",\n",
    "    \"InvestmentAmount\",\n",
    "]\n",
    "\n",
    "TARGET_COL = \"Risk Tolerance\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def load_data(path: str = DATA_CSV) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV into DataFrame.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data file not found at {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f55d642b-df78-4e9d-95aa-2a48d10f44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 2 — Stock summaries & comparison (compact)\n",
    "\n",
    "import numpy as _np\n",
    "\n",
    "def _cagr(s: pd.Series):\n",
    "    if s.empty: \n",
    "        return _np.nan\n",
    "    yrs = (s.index[-1] - s.index[0]).days / 365.25\n",
    "    return (s.iloc[-1] / s.iloc[0]) ** (1 / yrs) - 1 if yrs > 0 else _np.nan\n",
    "\n",
    "def _max_dd(s: pd.Series):\n",
    "    if s.empty: \n",
    "        return _np.nan\n",
    "    return ((s - s.cummax()) / s.cummax()).min()\n",
    "\n",
    "def summarize_stock_price_df(df: pd.DataFrame, price_col=\"Close\", name=\"STOCK\"):\n",
    "    if price_col not in df or df[price_col].dropna().empty:\n",
    "        return f\"### {name} — no price data.\"\n",
    "\n",
    "    s = df[price_col].dropna().sort_index()\n",
    "    r = s.pct_change().dropna()\n",
    "\n",
    "    return (\n",
    "        f\"### {name}\\n\"\n",
    "        f\"- Latest: {s.iloc[-1]:.2f}\\n\"\n",
    "        f\"- CAGR: {_cagr(s):.2%}\\n\"\n",
    "        f\"- Vol: {(r.std()*_np.sqrt(252)):.2%}\\n\"\n",
    "        f\"- Max DD: {_max_dd(s):.2%}\"\n",
    "    )\n",
    "\n",
    "def compare_two_price_series(df1, df2, price_col=\"Close\", name1=\"A\", name2=\"B\"):\n",
    "    \"\"\"\n",
    "    Clean, UI-friendly comparison of two assets.\n",
    "    Always returns readable Markdown (no raw dicts or dtype junk).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Series safely\n",
    "    def safe_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def metrics(df):\n",
    "        if df is None or df.empty or price_col not in df.columns:\n",
    "            return {\"ok\": False}\n",
    "\n",
    "        s = df[price_col].dropna()\n",
    "        if s.empty:\n",
    "            return {\"ok\": False}\n",
    "\n",
    "        last = safe_float(s.iloc[-1])\n",
    "\n",
    "        # CAGR\n",
    "        try:\n",
    "            cagr = _cagr(s)\n",
    "            cagr = float(cagr) if cagr is not None else None\n",
    "        except:\n",
    "            cagr = None\n",
    "\n",
    "        # Volatility\n",
    "        daily = s.pct_change().dropna()\n",
    "        vol = float(daily.std() * (252 ** 0.5)) if not daily.empty else None\n",
    "\n",
    "        # Max drawdown\n",
    "        try:\n",
    "            maxdd = _max_dd(s)\n",
    "            maxdd = float(maxdd) if maxdd is not None else None\n",
    "        except:\n",
    "            maxdd = None\n",
    "\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"last\": last,\n",
    "            \"cagr\": cagr,\n",
    "            \"vol\": vol,\n",
    "            \"maxdd\": maxdd,\n",
    "        }\n",
    "\n",
    "    # Compute metrics\n",
    "    m1 = metrics(df1)\n",
    "    m2 = metrics(df2)\n",
    "\n",
    "    # Build Markdown\n",
    "    md = []\n",
    "    md.append(f\"## Comparison: {name1} vs {name2}\")\n",
    "\n",
    "    if not m1[\"ok\"] or not m2[\"ok\"]:\n",
    "        md.append(\"**Not enough price data to compare these two symbols.**\")\n",
    "        return \"\\n\".join(md)\n",
    "\n",
    "    # Summary table\n",
    "    md.append(\"### Snapshot\\n\")\n",
    "    md.append(f\"- **{name1}**: Price ₹{m1['last']:.2f}, CAGR {m1['cagr']*100:.2f}%, Vol {m1['vol']*100:.2f}%, MaxDD {m1['maxdd']*100:.2f}%\")\n",
    "    md.append(f\"- **{name2}**: Price ₹{m2['last']:.2f}, CAGR {m2['cagr']*100:.2f}%, Vol {m2['vol']*100:.2f}%, MaxDD {m2['maxdd']*100:.2f}%\")\n",
    "\n",
    "    # Pros & cons\n",
    "    md.append(\"\\n### Pros & Cons\\n\")\n",
    "\n",
    "    # Pros A\n",
    "    pros1 = []\n",
    "    if m1[\"cagr\"] > m2[\"cagr\"]:\n",
    "        pros1.append(\"Higher returns (CAGR).\")\n",
    "    if m1[\"vol\"] < m2[\"vol\"]:\n",
    "        pros1.append(\"Lower volatility.\")\n",
    "    if m1[\"maxdd\"] > m2[\"maxdd\"]:\n",
    "        pros1.append(\"Smaller drawdowns.\")\n",
    "\n",
    "    cons1 = []\n",
    "    if m1[\"cagr\"] < m2[\"cagr\"]:\n",
    "        cons1.append(\"Lower returns (CAGR).\")\n",
    "    if m1[\"vol\"] > m2[\"vol\"]:\n",
    "        cons1.append(\"Higher volatility.\")\n",
    "    if m1[\"maxdd\"] < m2[\"maxdd\"]:\n",
    "        cons1.append(\"Larger drawdowns.\")\n",
    "\n",
    "    # Pros B\n",
    "    pros2 = []\n",
    "    if m2[\"cagr\"] > m1[\"cagr\"]:\n",
    "        pros2.append(\"Higher returns (CAGR).\")\n",
    "    if m2[\"vol\"] < m1[\"vol\"]:\n",
    "        pros2.append(\"Lower volatility.\")\n",
    "    if m2[\"maxdd\"] > m1[\"maxdd\"]:\n",
    "        pros2.append(\"Smaller drawdowns.\")\n",
    "\n",
    "    cons2 = []\n",
    "    if m2[\"cagr\"] < m1[\"cagr\"]:\n",
    "        cons2.append(\"Lower returns (CAGR).\")\n",
    "    if m2[\"vol\"] > m1[\"vol\"]:\n",
    "        cons2.append(\"Higher volatility.\")\n",
    "    if m2[\"maxdd\"] < m1[\"maxdd\"]:\n",
    "        cons2.append(\"Larger drawdowns.\")\n",
    "\n",
    "    md.append(f\"#### {name1} Pros\\n\" + (\"\\n\".join(f\"- {x}\" for x in pros1) if pros1 else \"- None\"))\n",
    "    md.append(f\"#### {name1} Cons\\n\" + (\"\\n\".join(f\"- {x}\" for x in cons1) if cons1 else \"- None\"))\n",
    "\n",
    "    md.append(f\"\\n#### {name2} Pros\\n\" + (\"\\n\".join(f\"- {x}\" for x in pros2) if pros2 else \"- None\"))\n",
    "    md.append(f\"#### {name2} Cons\\n\" + (\"\\n\".join(f\"- {x}\" for x in cons2) if cons2 else \"- None\"))\n",
    "\n",
    "    # Verdict\n",
    "    score1 = (m1[\"cagr\"] > m2[\"cagr\"]) + (m1[\"vol\"] < m2[\"vol\"]) + (m1[\"maxdd\"] > m2[\"maxdd\"])\n",
    "    score2 = (m2[\"cagr\"] > m1[\"cagr\"]) + (m2[\"vol\"] < m1[\"vol\"]) + (m2[\"maxdd\"] > m1[\"maxdd\"])\n",
    "\n",
    "    verdict = name1 if score1 > score2 else name2 if score2 > score1 else \"Tie\"\n",
    "\n",
    "    md.append(f\"\\n### Final Verdict\\n**{verdict}** looks better overall for a conservative investor.\")\n",
    "\n",
    "    return \"\\n\".join(md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c092060-ad6f-45dc-8c43-aaf793e11ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Block 3 — XGBoost training + predict (UI-aligned)\n",
    "\n",
    "\n",
    "def _age_to_group(age):\n",
    "    try:\n",
    "        a = float(age)\n",
    "    except:\n",
    "        return \"26-35\"\n",
    "    if a < 26:\n",
    "        return \"18-25\"\n",
    "    elif a < 36:\n",
    "        return \"26-35\"\n",
    "    elif a < 46:\n",
    "        return \"36-45\"\n",
    "    elif a < 61:\n",
    "        return \"46-60\"\n",
    "    else:\n",
    "        return \"60+\"\n",
    "\n",
    "def _income_to_group(inc):\n",
    "    try:\n",
    "        v = float(inc)\n",
    "    except:\n",
    "        return \"30,000-70,000\"\n",
    "    if v < 30000:\n",
    "        return \"<30,000\"\n",
    "    elif v <= 70000:\n",
    "        return \"30,000-70,000\"\n",
    "    else:\n",
    "        return \"70,000+\"\n",
    "\n",
    "def _ensure_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add simplified/grouped features that UI uses.\"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    if \"AgeGroup\" not in d:\n",
    "        d[\"AgeGroup\"] = d[\"Age\"].apply(_age_to_group) if \"Age\" in d else \"26-35\"\n",
    "\n",
    "    if \"IncomeGroup\" not in d:\n",
    "        d[\"IncomeGroup\"] = d[\"Income Level\"].apply(_income_to_group) if \"Income Level\" in d else \"30,000-70,000\"\n",
    "\n",
    "    d[\"EmploymentStatus\"] = d.get(\"Employment Status\", \"Salaried\").astype(str)\n",
    "    d[\"LoanStatus\"] = d.get(\"Loan Status\", \"No\").astype(str)\n",
    "\n",
    "    if \"InvestmentGoal\" not in d:\n",
    "        d[\"InvestmentGoal\"] = d.get(\"Investment Goals\", \"Growth\").astype(str)\n",
    "\n",
    "    if \"InvestmentAmount\" not in d:\n",
    "        if \"Investments\" in d:\n",
    "            d[\"InvestmentAmount\"] = d[\"Investments\"].fillna(0)\n",
    "        elif \"Net Savings\" in d:\n",
    "            d[\"InvestmentAmount\"] = d[\"Net Savings\"].fillna(0)\n",
    "        elif \"Account Balance\" in d:\n",
    "            d[\"InvestmentAmount\"] = d[\"Account Balance\"].fillna(0)\n",
    "        else:\n",
    "            d[\"InvestmentAmount\"] = 0.0\n",
    "\n",
    "    return d\n",
    "\n",
    "def preprocess(df: pd.DataFrame):\n",
    "    \"\"\"Prepare X: keep rich numeric features + encode grouped ones.\"\"\"\n",
    "    df = _ensure_features(df)\n",
    "\n",
    "    use_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
    "    if not use_cols:\n",
    "        raise ValueError(\"None of the FEATURE_COLS found in dataset\")\n",
    "\n",
    "    X = df[use_cols].copy()\n",
    "\n",
    "    # Encode categoricals\n",
    "    cat_maps = {}\n",
    "    for c in X.select_dtypes(include=[\"object\"]).columns:\n",
    "        X[c] = X[c].fillna(\"missing\").astype(\"category\")\n",
    "        cat_maps[c] = list(X[c].cat.categories)\n",
    "        X[c] = X[c].cat.codes\n",
    "\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X.loc[:, :] = scaler.fit_transform(X.values)\n",
    "\n",
    "    return X, cat_maps, scaler\n",
    "\n",
    "def train_xgb_model(df: pd.DataFrame):\n",
    "    \"\"\"Train XGBoost and print accuracy.\"\"\"\n",
    "    if TARGET_COL not in df.columns:\n",
    "        raise ValueError(f\"CSV must contain '{TARGET_COL}'\")\n",
    "\n",
    "    X, cat_maps, scaler = preprocess(df)\n",
    "    label_enc = LabelEncoder()\n",
    "    y = label_enc.fit_transform(df[TARGET_COL].astype(str))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=350,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        use_label_encoder=False,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"XGBoost test accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, preds, target_names=label_enc.classes_))\n",
    "\n",
    "    bundle = {\n",
    "        \"model\": model,\n",
    "        \"columns\": X.columns.tolist(),\n",
    "        \"cat_maps\": cat_maps,\n",
    "        \"scaler\": scaler,\n",
    "        \"label_encoder\": label_enc,\n",
    "    }\n",
    "    joblib.dump(bundle, XGB_MODEL_PATH)\n",
    "    print(f\"Saved model to {XGB_MODEL_PATH}\")\n",
    "    return bundle\n",
    "\n",
    "def load_xgb_model(path: str = XGB_MODEL_PATH):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\"Model not trained yet.\")\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_risk(bundle: Dict[str, Any], inp: Dict[str, Any]):\n",
    "    \"\"\"Predict risk tolerance for one user profile.\"\"\"\n",
    "    cols = bundle[\"columns\"]\n",
    "    cat_maps = bundle[\"cat_maps\"]\n",
    "    scaler = bundle[\"scaler\"]\n",
    "    le: LabelEncoder = bundle[\"label_encoder\"]\n",
    "    model = bundle[\"model\"]\n",
    "\n",
    "    row = {c: inp.get(c, \"missing\") for c in cols}\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    for col, cats in cat_maps.items():\n",
    "        val = df_row.at[0, col]\n",
    "        if val not in cats:\n",
    "            val = \"missing\" if \"missing\" in cats else cats[0]\n",
    "        df_row[col] = cats.index(val)\n",
    "\n",
    "    df_row = df_row.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "    df_row[cols] = scaler.transform(df_row[cols].values)\n",
    "\n",
    "    proba = model.predict_proba(df_row)[0]\n",
    "    idx = int(np.argmax(proba))\n",
    "\n",
    "    return {\n",
    "        \"prediction\": le.inverse_transform([idx])[0],\n",
    "        \"probability\": float(proba[idx]),\n",
    "        \"class_probabilities\": {label: float(p) for label, p in zip(le.classes_, proba)},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6aee58-6aab-48c9-bb4f-2c5c0b6bd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     df = load_data()\n",
    "#     train_xgb_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f36c2c7-9a0a-449e-952b-54431d882ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 4 — ChromaDB + embeddings setup\n",
    "\n",
    "\n",
    "# Required from Block 1 — add defaults here to avoid NameError\n",
    "try:\n",
    "    CHROMA_COLLECTION_NAME\n",
    "except NameError:\n",
    "    CHROMA_COLLECTION_NAME = \"fin_docs\"\n",
    "\n",
    "try:\n",
    "    CHROMA_DIR\n",
    "except NameError:\n",
    "    CHROMA_DIR = r\"C:\\Users\\rahil\\Downloads\\chroma_db\"\n",
    "\n",
    "try:\n",
    "    EMBEDDING_MODEL_NAME\n",
    "except NameError:\n",
    "    EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "try:\n",
    "    DEFAULT_TOP_K\n",
    "except NameError:\n",
    "    DEFAULT_TOP_K = 4\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def init_chroma(collection_name: str = CHROMA_COLLECTION_NAME, persist_dir: str = CHROMA_DIR):\n",
    "    \"\"\"\n",
    "    Initialize or load a ChromaDB collection.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    try:\n",
    "        coll = client.get_collection(name=collection_name)\n",
    "        print(f\"Loaded existing Chroma collection: {collection_name}\")\n",
    "    except Exception:\n",
    "        coll = client.create_collection(name=collection_name)\n",
    "        print(f\"Created new Chroma collection: {collection_name}\")\n",
    "\n",
    "    return client, coll\n",
    "\n",
    "\n",
    "def build_embeddings_and_upsert(docs, collection, embed_model_name: str = EMBEDDING_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Encode text and insert into ChromaDB.\n",
    "    Each doc: {\"id\": \"...\", \"text\": \"...\", \"metadata\": {...}}\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    ids = [d[\"id\"] for d in docs]\n",
    "    metas = [d.get(\"metadata\", {}) for d in docs]\n",
    "\n",
    "    embs = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        metadatas=metas,\n",
    "        embeddings=embs.tolist(),\n",
    "    )\n",
    "\n",
    "    print(f\"Upserted {len(ids)} documents into collection '{collection.name}'\")\n",
    "\n",
    "\n",
    "def make_retriever(k: int = DEFAULT_TOP_K):\n",
    "    \"\"\"\n",
    "    Create retriever for similarity-based queries.\n",
    "    \"\"\"\n",
    "    hf = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    vect_store = Chroma(\n",
    "        collection_name=CHROMA_COLLECTION_NAME,\n",
    "        embedding_function=hf,\n",
    "        persist_directory=CHROMA_DIR,\n",
    "    )\n",
    "\n",
    "    return vect_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f9188a0-aefa-4dbd-941c-5f2d4532e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5 - RAG: vector search + live stock/news fetch\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# looser ticker pattern (letters, numbers, dots, hyphens, up to 10 chars)\n",
    "_TICKER_RE = re.compile(r\"^[A-Za-z0-9\\.\\-]{1,10}$\")\n",
    "\n",
    "\n",
    "def fetch_stock_data(ticker: str) -> Document | None:\n",
    "    \"\"\"\n",
    "    Get a short stock summary from yfinance and wrap it in a Document.\n",
    "    Returns None if no data was found or on error.\n",
    "    \"\"\"\n",
    "    ticker = str(ticker).strip().upper()\n",
    "    if not _TICKER_RE.match(ticker):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        hist = stock.history(period=\"1mo\")\n",
    "        if hist is None or hist.empty:\n",
    "            return None\n",
    "\n",
    "        last_close = float(hist[\"Close\"].iloc[-1])\n",
    "        avg_5 = float(hist[\"Close\"].tail(5).mean())\n",
    "\n",
    "        sector = None\n",
    "        industry = None\n",
    "        try:\n",
    "            info = stock.info if isinstance(stock.info, dict) else {}\n",
    "            sector = info.get(\"sector\")\n",
    "            industry = info.get(\"industry\")\n",
    "        except Exception:\n",
    "            # some tickers / yfinance versions can raise when accessing .info\n",
    "            sector = None\n",
    "            industry = None\n",
    "\n",
    "        text = (\n",
    "            f\"{ticker} latest close: {last_close:.2f}. \"\n",
    "            f\"5-day avg close: {avg_5:.2f}. \"\n",
    "            f\"Sector: {sector or 'N/A'}. \"\n",
    "            f\"Industry: {industry or 'N/A'}.\"\n",
    "        )\n",
    "        return Document(page_content=text, metadata={\"source\": \"yfinance\", \"ticker\": ticker})\n",
    "    except Exception as e:\n",
    "        print(f\"[fetch_stock_data] error for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def rag_query(query: str, retriever, collection, k: int | None = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Query retriever (if present). If no relevant docs, fetch live data\n",
    "    (yfinance + news) and upsert into collection (if upsert function present).\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = globals().get(\"DEFAULT_TOP_K\", 4)\n",
    "\n",
    "    # Try retrieval (pass k where possible)\n",
    "    results = []\n",
    "    if retriever is not None:\n",
    "        try:\n",
    "            # prefer retriever.get_relevant_documents(query, k=k) if signature supports it\n",
    "            try:\n",
    "                results = retriever.get_relevant_documents(query, k=k)\n",
    "            except TypeError:\n",
    "                # older retriever signature\n",
    "                results = retriever.get_relevant_documents(query)\n",
    "        except Exception as exc:\n",
    "            print(f\"[rag_query] retriever error: {exc}\")\n",
    "            results = []\n",
    "\n",
    "    # Quick relevance check\n",
    "    if results:\n",
    "        try:\n",
    "            top_text = (results[0].page_content or \"\").lower()\n",
    "            meta_text = str(results[0].metadata or \"\").lower()\n",
    "            if query.lower() in top_text or query.lower() in meta_text:\n",
    "                print(f\"[rag_query] found {len(results)} relevant docs for '{query}'\")\n",
    "                return results\n",
    "            else:\n",
    "                print(f\"[rag_query] docs found but not clearly relevant for '{query}' — fetching live data\")\n",
    "        except Exception:\n",
    "            print(\"[rag_query] could not inspect retrieved docs; fetching live data\")\n",
    "\n",
    "    #  Live fetch fallback \n",
    "    print(f\"[rag_query] fetching live data for '{query}' (yfinance + news)...\")\n",
    "    new_docs: List[Document] = []\n",
    "\n",
    "    q_clean = query.strip()\n",
    "    # If query looks like a ticker, try stock summary\n",
    "    if _TICKER_RE.match(q_clean):\n",
    "        sd = fetch_stock_data(q_clean.upper())\n",
    "        if sd:\n",
    "            new_docs.append(sd)\n",
    "\n",
    "    # Fetch news via Tavily (caller must implement actual API call)\n",
    "    tavily_results = None  # <-- replace with real call where available\n",
    "    try:\n",
    "        news_docs = fetch_news_tavily(query, max_results=5, results=tavily_results)\n",
    "        new_docs.extend(news_docs)\n",
    "    except Exception as exc:\n",
    "        print(f\"[rag_query] fetch_news_tavily failed: {exc}\")\n",
    "\n",
    "    # Upsert into collection if possible\n",
    "    if new_docs:\n",
    "        upsert_items = [{\"id\": f\"live_{i}_{q_clean}\", \"text\": d.page_content, \"metadata\": d.metadata} for i, d in enumerate(new_docs)]\n",
    "        upsert_fn = globals().get(\"build_embeddings_and_upsert\")\n",
    "        if callable(upsert_fn) and collection is not None:\n",
    "            try:\n",
    "                upsert_fn(upsert_items, collection)\n",
    "            except Exception as exc:\n",
    "                print(f\"[rag_query] upsert failed: {exc}\")\n",
    "\n",
    "    return new_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c60421-67f2-466f-9723-834834fdf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Tools for the agent\n",
    "\n",
    "from langchain.agents import Tool\n",
    "\n",
    "\n",
    "# Risk profiling tool\n",
    "def risk_tool_func(user_input: dict):\n",
    "    \"\"\"\n",
    "    Predict the user's risk tolerance using the stored XGBoost model.\n",
    "    Expects keys like AgeGroup, IncomeGroup, EmploymentStatus, LoanStatus,\n",
    "    InvestmentGoal, InvestmentAmount (aligned with predict_risk).\n",
    "    \"\"\"\n",
    "    bundle = load_xgb_model()\n",
    "    return predict_risk(bundle, user_input)\n",
    "\n",
    "\n",
    "risk_tool = Tool(\n",
    "    name=\"RiskProfiler\",\n",
    "    func=risk_tool_func,\n",
    "    description=(\n",
    "        \"Predicts a user's risk tolerance (High, Medium, Low) based on \"\n",
    "        \"their financial profile (age group, income group, employment, loans, goals, etc.).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Market data / RAG tool\n",
    "def market_tool_func(query: str):\n",
    "    \"\"\"\n",
    "    Returns market or stock-related context from the RAG setup.\n",
    "    \"\"\"\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "    docs = rag_query(query, retriever, coll) or []\n",
    "    if not docs:\n",
    "        return \"No market information found.\"\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "market_tool = Tool(\n",
    "    name=\"MarketData\",\n",
    "    func=market_tool_func,\n",
    "    description=(\n",
    "        \"Provides market/stock context from the internal RAG store. \"\n",
    "        \"Accepts a stock ticker (e.g. AAPL) or company name (e.g. Apple).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Stock comparison tool\n",
    "def compare_tool_func(stock1: str, stock2: str):\n",
    "    \"\"\"\n",
    "    Fetch RAG summaries for two stocks and return a combined view\n",
    "    for the LLM to analyse further (pros/cons, verdict).\n",
    "    \"\"\"\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "\n",
    "    docs1 = rag_query(stock1, retriever, coll) or []\n",
    "    docs2 = rag_query(stock2, retriever, coll) or []\n",
    "\n",
    "    summary1 = \"\\n\\n\".join(d.page_content for d in docs1) or \"No data found.\"\n",
    "    summary2 = \"\\n\\n\".join(d.page_content for d in docs2) or \"No data found.\"\n",
    "\n",
    "    return (\n",
    "        f\"Stock 1: {stock1}\\n{summary1}\\n\\n\"\n",
    "        f\"Stock 2: {stock2}\\n{summary2}\\n\\n\"\n",
    "        \"Now compare them with pros/cons and a final verdict.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _compare_wrapper(pair: str):\n",
    "    # handle 'AAPL,MSFT' or 'AAPL, MSFT'\n",
    "    parts = [p.strip() for p in pair.split(\",\")]\n",
    "    if len(parts) != 2:\n",
    "        return \"Please provide two tickers as 'TICKER1,TICKER2'.\"\n",
    "    return compare_tool_func(parts[0], parts[1])\n",
    "\n",
    "\n",
    "compare_tool = Tool(\n",
    "    name=\"CompareStocks\",\n",
    "    func=_compare_wrapper,\n",
    "    description=\"Compare two stocks by providing input as 'TICKER1,TICKER2'.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c79fe285-2fa7-4f11-8b2f-0086b0c6b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Agent setup with LangChain\n",
    "\n",
    "import re\n",
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def parse_user_profile(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse simple 'key=value' pairs (comma-separated) into a dictionary.\n",
    "\n",
    "    Example:\n",
    "        \"Age=30, Occupation=Engineer, Income=60000\"\n",
    "    \"\"\"\n",
    "    profile = {}\n",
    "    for part in text.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part or \"=\" not in part:\n",
    "            continue\n",
    "        key, val = part.split(\"=\", 1)\n",
    "        key = key.strip()\n",
    "        val = val.strip()\n",
    "        # Try to convert numeric values\n",
    "        try:\n",
    "            val = float(val) if \".\" in val else int(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        profile[key] = val\n",
    "    return profile\n",
    "\n",
    "\n",
    "# 1) Risk profiling tool\n",
    "def risk_tool_func(user_input: str):\n",
    "    \"\"\"\n",
    "    Accepts text like:\n",
    "      \"Age=30, Income=60000, Employment Status=Salaried, Loan Amount=10000, Investment Goals=Growth, Investments=25000\"\n",
    "    and maps it into the simplified feature space expected by predict_risk.\n",
    "    \"\"\"\n",
    "    bundle = load_xgb_model()\n",
    "    raw = parse_user_profile(user_input)\n",
    "\n",
    "    # Map raw values into grouped features used by the model\n",
    "    age_val = raw.get(\"Age\", raw.get(\"age\", 30))\n",
    "    inc_val = raw.get(\"Income\", raw.get(\"Income Level\", raw.get(\"income\", 60000)))\n",
    "\n",
    "    age_group = _age_to_group(age_val)\n",
    "    income_group = _income_to_group(inc_val)\n",
    "\n",
    "    emp = raw.get(\"Employment Status\", raw.get(\"EmploymentStatus\", \"Salaried\"))\n",
    "    loan_status_raw = str(raw.get(\"Loan Status\", raw.get(\"LoanStatus\", \"\"))).lower()\n",
    "    loan_amt = float(raw.get(\"Loan Amount\", raw.get(\"loan_amount\", 0)) or 0)\n",
    "    has_loan = loan_amt > 0 or loan_status_raw in [\"yes\", \"y\", \"true\", \"1\"]\n",
    "    loan_status = \"Yes\" if has_loan else \"No\"\n",
    "\n",
    "    goal = raw.get(\"Investment Goals\", raw.get(\"Goal\", raw.get(\"InvestmentGoal\", \"Growth\")))\n",
    "    invest_amt = raw.get(\n",
    "        \"InvestmentAmount\",\n",
    "        raw.get(\"Investments\", raw.get(\"Net Savings\", raw.get(\"Account Balance\", 0.0))),\n",
    "    )\n",
    "    try:\n",
    "        invest_amt = float(invest_amt)\n",
    "    except Exception:\n",
    "        invest_amt = 0.0\n",
    "\n",
    "    feature_row = {\n",
    "        \"AgeGroup\": age_group,\n",
    "        \"IncomeGroup\": income_group,\n",
    "        \"EmploymentStatus\": str(emp),\n",
    "        \"LoanStatus\": loan_status,\n",
    "        \"InvestmentGoal\": str(goal),\n",
    "        \"InvestmentAmount\": invest_amt,\n",
    "    }\n",
    "\n",
    "    return predict_risk(bundle, feature_row)\n",
    "\n",
    "\n",
    "risk_tool = Tool(\n",
    "    name=\"RiskProfiler\",\n",
    "    func=risk_tool_func,\n",
    "    description=(\n",
    "        \"Predicts a user's risk tolerance (High, Medium, Low) from profile data. \"\n",
    "        \"Input format: key=value pairs separated by commas. \"\n",
    "        \"Example: 'Age=30, Income=60000, Employment Status=Salaried, \"\n",
    "        \"Loan Amount=10000, Investment Goals=Growth, Investments=25000'.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Market data tool (RAG over your vector store)\n",
    "def market_tool_func(query: str):\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "    docs = rag_query(query, retriever, coll) or []\n",
    "    if not docs:\n",
    "        return \"No market context found for this query.\"\n",
    "    return \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "market_tool = Tool(\n",
    "    name=\"MarketData\",\n",
    "    func=market_tool_func,\n",
    "    description=(\n",
    "        \"Returns market-related context from the internal RAG setup. \"\n",
    "        \"Input can be a stock ticker (AAPL, TSLA) or a company name (Apple, Tesla).\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Comparison tool\n",
    "def compare_tool_func(query: str):\n",
    "    \"\"\"\n",
    "    Compare two stocks given as 'AAPL,MSFT' or text like 'Compare Apple and Microsoft'.\n",
    "    \"\"\"\n",
    "    query = query.strip()\n",
    "\n",
    "    # Try comma-separated first\n",
    "    if \",\" in query:\n",
    "        parts = [x.strip() for x in query.split(\",\") if x.strip()]\n",
    "        if len(parts) != 2:\n",
    "            return \"Please provide exactly two stocks, e.g. 'AAPL,MSFT'.\"\n",
    "        stock1, stock2 = parts\n",
    "    else:\n",
    "        # Fallback: split on 'and'\n",
    "        parts = re.split(r\"\\band\\b\", query, flags=re.IGNORECASE)\n",
    "        if len(parts) == 2:\n",
    "            stock1, stock2 = parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            return \"Please provide two stocks, e.g. 'AAPL,MSFT'.\"\n",
    "\n",
    "    client, coll = init_chroma()\n",
    "    retriever = make_retriever()\n",
    "    docs1 = rag_query(stock1, retriever, coll) or []\n",
    "    docs2 = rag_query(stock2, retriever, coll) or []\n",
    "\n",
    "    summary1 = \"\\n\".join(d.page_content for d in docs1) or \"No context found.\"\n",
    "    summary2 = \"\\n\".join(d.page_content for d in docs2) or \"No context found.\"\n",
    "\n",
    "    return (\n",
    "        f\"Stock 1: {stock1}\\n{summary1}\\n\\n\"\n",
    "        f\"Stock 2: {stock2}\\n{summary2}\\n\\n\"\n",
    "        \"Now provide pros/cons and a final verdict.\"\n",
    "    )\n",
    "\n",
    "\n",
    "compare_tool = Tool(\n",
    "    name=\"CompareStocks\",\n",
    "    func=compare_tool_func,\n",
    "    description=\"Compare two stocks. Input: 'AAPL,MSFT' or 'Compare Apple and Microsoft'.\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_agent():\n",
    "    \"\"\"\n",
    "    Create a LangChain agent wired up with risk, market, and comparison tools.\n",
    "    \"\"\"\n",
    "    if not OPENAI_KEY:\n",
    "        raise ValueError(\"Missing OPENAI_API_KEY.\")\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.2,\n",
    "        openai_api_key=OPENAI_KEY,\n",
    "    )\n",
    "\n",
    "    tools = [risk_tool, market_tool, compare_tool]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "    )\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb4b8365-6aa9-4002-b908-5e6895bc5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "# agent = get_agent()\n",
    "\n",
    "# # 1. Risk profiling\n",
    "# query1 = \"Age=30, Occupation=Engineer, Income=60000, Loan Amount=10000, Employment Status=Employed, Investment Goals=Growth\"\n",
    "# print(agent.run(query1))\n",
    "\n",
    "# # 2. Market data\n",
    "# query2 = \"Tell me about Tesla\"\n",
    "# print(agent.run(query2))\n",
    "\n",
    "# # 3. Compare stocks\n",
    "# query3 = \"Compare AAPL and MSFT\"\n",
    "# print(agent.run(query3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ad82b9a-4c83-407f-96f0-8f4301a12e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8.1 - Prompt + LLM chain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "# Ensure the key exists (fallback to env directly)\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "profile_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "You're a helpful financial assistant. Read the user's profile text and return a JSON object with these keys:\n",
    "- Age Group: one of \"18-25\", \"26-35\", \"36-45\", \"46-60\", \"60+\"\n",
    "- Income Group: one of \"<30,000\", \"30,000-70,000\", \"70,000+\"\n",
    "- Employment Status: e.g. \"Salaried\", \"Self-employed\", \"Retired\"\n",
    "- Loan Status: one of \"approved\", \"pending\", \"rejected\"\n",
    "- Investment Goal: e.g. \"Growth\", \"Wealth Preservation\", \"Short-term Safety\"\n",
    "- Investment Amount: numeric (INR)\n",
    "\n",
    "User text: {text}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm_client = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=OPENAI_KEY\n",
    ")\n",
    "\n",
    "profile_parser = LLMChain(llm=llm_client, prompt=profile_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0895cb48-939c-453e-af07-605d1f4e4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 - Allocation and projection utilities\n",
    "def allocation_for_risk(risk: str, goal: str):\n",
    "    if risk == \"High\":\n",
    "        return {\"Stocks\": 0.70, \"FD\": 0.20, \"Gold\": 0.10}\n",
    "    if risk == \"Medium\":\n",
    "        return {\"Stocks\": 0.50, \"FD\": 0.30, \"Gold\": 0.20}\n",
    "    return {\"Stocks\": 0.30, \"FD\": 0.40, \"Gold\": 0.30}\n",
    "\n",
    "def simulate_growth(principal: float, split: dict, years: int = 5):\n",
    "    rates = {\"Stocks\": 0.12, \"FD\": 0.06, \"Gold\": 0.08}\n",
    "    portfolio_vals = [principal]\n",
    "    for _ in range(years):\n",
    "        prev = portfolio_vals[-1]\n",
    "        next_total = sum(prev * split[k] * (1 + rates[k]) for k in split)\n",
    "        portfolio_vals.append(next_total)\n",
    "    fd_series = [principal * ((1 + rates[\"FD\"])**i) for i in range(years+1)]\n",
    "    gold_series = [principal * ((1 + rates[\"Gold\"])**i) for i in range(years+1)]\n",
    "    return portfolio_vals, fd_series, gold_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ddb0d19e-e406-4eef-b76d-73469039ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 - Fetch history + basic indicators (with NSE .NS fallback)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_history(ticker: str, period: str = \"6mo\", interval: str = \"1d\"):\n",
    "    \"\"\"\n",
    "    Fetch price history for a symbol.\n",
    "\n",
    "    - First tries the raw symbol (e.g. INFY, AAPL).\n",
    "    - If that returns no data and the symbol has no suffix,\n",
    "      also tries '<SYMBOL>.NS' for common Indian stocks (TCS, RELIANCE, etc.).\n",
    "    \"\"\"\n",
    "    base = (ticker or \"\").strip()\n",
    "    if not base:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    candidates = [base]\n",
    "    if \".\" not in base:  # no exchange suffix provided -> also try NSE\n",
    "        candidates.append(base.upper() + \".NS\")\n",
    "\n",
    "    for sym in candidates:\n",
    "        try:\n",
    "            df = yf.Ticker(sym).history(period=period, interval=interval)\n",
    "            if df is not None and not df.empty:\n",
    "                df = df.reset_index()\n",
    "\n",
    "                # Ensure there is a 'Date' column for plotting\n",
    "                if \"Date\" not in df.columns:\n",
    "                    # yfinance usually names the index 'Date', but just in case:\n",
    "                    df.rename(columns={df.columns[0]: \"Date\"}, inplace=True)\n",
    "\n",
    "                # Optional: keep track of which symbol actually worked\n",
    "                df[\"__symbol__\"] = sym\n",
    "                return df\n",
    "        except Exception as exc:\n",
    "            print(f\"[get_history] yfinance error for {sym}: {exc}\")\n",
    "\n",
    "    # If nothing worked, return empty frame\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def add_indicators(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Add SMA20, EMA20, RSI14, MACD, MACD_Signal columns to a price DataFrame.\n",
    "    Expects a 'Close' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Close\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # SMA / EMA\n",
    "    df[\"SMA20\"] = df[\"Close\"].rolling(20, min_periods=1).mean()\n",
    "    df[\"EMA20\"] = df[\"Close\"].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "    # RSI14 (standard EMA-based)\n",
    "    diff = df[\"Close\"].diff()\n",
    "    up = diff.clip(lower=0)\n",
    "    down = -diff.clip(upper=0)\n",
    "    ma_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ma_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ma_up / (ma_down + 1e-9)\n",
    "    df[\"RSI14\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD\n",
    "    ema12 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = ema12 - ema26\n",
    "    df[\"MACD_Signal\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_with_indicators(df: pd.DataFrame, ticker: str):\n",
    "    \"\"\"\n",
    "    Plot price + SMA20/EMA20, RSI14, and MACD for a given history DataFrame.\n",
    "    Returns a matplotlib Figure (or None if df is empty).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or \"Close\" not in df.columns or \"Date\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    df2 = add_indicators(df)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(8, 8))\n",
    "    gs = fig.add_gridspec(3, 1, height_ratios=[3, 1, 1])\n",
    "\n",
    "    # Price + MAs\n",
    "    ax_price = fig.add_subplot(gs[0, 0])\n",
    "    ax_price.plot(df2[\"Date\"], df2[\"Close\"], lw=1, label=\"Close\")\n",
    "    ax_price.plot(df2[\"Date\"], df2[\"SMA20\"], lw=1, label=\"SMA20\")\n",
    "    ax_price.plot(df2[\"Date\"], df2[\"EMA20\"], lw=1, label=\"EMA20\")\n",
    "    ax_price.set_title(f\"{ticker} — Price with SMA/EMA\")\n",
    "    ax_price.legend(loc=\"upper left\")\n",
    "\n",
    "    # RSI\n",
    "    ax_rsi = fig.add_subplot(gs[1, 0], sharex=ax_price)\n",
    "    ax_rsi.plot(df2[\"Date\"], df2[\"RSI14\"], label=\"RSI14\")\n",
    "    ax_rsi.axhline(70, linestyle=\"--\", linewidth=0.7)\n",
    "    ax_rsi.axhline(30, linestyle=\"--\", linewidth=0.7)\n",
    "    ax_rsi.set_ylabel(\"RSI\")\n",
    "\n",
    "    # MACD\n",
    "    ax_macd = fig.add_subplot(gs[2, 0], sharex=ax_price)\n",
    "    ax_macd.plot(df2[\"Date\"], df2[\"MACD\"], label=\"MACD\")\n",
    "    ax_macd.plot(df2[\"Date\"], df2[\"MACD_Signal\"], label=\"Signal\")\n",
    "    ax_macd.set_ylabel(\"MACD\")\n",
    "    ax_macd.legend(loc=\"upper left\")\n",
    "\n",
    "    for lbl in ax_macd.get_xticklabels():\n",
    "        lbl.set_rotation(30)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8f5324e-5681-4441-a4fb-2eccb1e60b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 - Risk profiler runner\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_risk_profile(user_text: str,\n",
    "                     age_sel, income_sel, employment_sel,\n",
    "                     loan_sel, goal_sel, invest_amount):\n",
    "    # initial status\n",
    "    yield (\"Parsing / running...\", \"\", None, None)\n",
    "\n",
    "    parsed = None\n",
    "    parse_note = None\n",
    "\n",
    "    # 1) Try free-text LLM parser\n",
    "    if user_text and user_text.strip():\n",
    "        try:\n",
    "            raw = profile_parser.run({\"text\": user_text})\n",
    "            parsed = json.loads(raw)\n",
    "        except Exception as exc:\n",
    "            parsed = None\n",
    "            parse_note = f\"Parsing failed: {exc} — using form values.\"\n",
    "\n",
    "    # 2) Start from form values\n",
    "    age_group = age_sel\n",
    "    income_group = income_sel\n",
    "    employment = employment_sel\n",
    "    loan_status = loan_sel\n",
    "    goal = goal_sel\n",
    "    try:\n",
    "        amount = float(invest_amount or 0)\n",
    "    except Exception:\n",
    "        amount = 0.0\n",
    "\n",
    "    # 3) If parsing worked, override with parsed fields (LLM keys → model keys)\n",
    "    if isinstance(parsed, dict):\n",
    "        age_group = parsed.get(\"Age Group\", age_group)\n",
    "        income_group = parsed.get(\"Income Group\", income_group)\n",
    "        employment = parsed.get(\"Employment Status\", employment)\n",
    "        loan_status = parsed.get(\"Loan Status\", loan_status)\n",
    "        goal = parsed.get(\"Investment Goal\", goal)\n",
    "        try:\n",
    "            amount = float(parsed.get(\"Investment Amount\", amount) or 0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 4) Normalise loan status to approved / pending / rejected\n",
    "    loan_str = str(loan_status).strip().lower()\n",
    "    if \"approve\" in loan_str:\n",
    "        loan_norm = \"approved\"\n",
    "    elif \"pend\" in loan_str:\n",
    "        loan_norm = \"pending\"\n",
    "    elif \"reject\" in loan_str:\n",
    "        loan_norm = \"rejected\"\n",
    "    else:\n",
    "        # fall back to dropdown value (should already be one of the three)\n",
    "        loan_norm = str(loan_sel)\n",
    "\n",
    "    # 5) Build feature dict exactly as the XGBoost pipeline expects\n",
    "    features = {\n",
    "        \"AgeGroup\": age_group,\n",
    "        \"IncomeGroup\": income_group,\n",
    "        \"EmploymentStatus\": employment,\n",
    "        \"InvestmentGoal\": goal,\n",
    "        \"LoanStatus\": loan_norm,\n",
    "        \"InvestmentAmount\": amount,\n",
    "    }\n",
    "\n",
    "    # 6) Model prediction\n",
    "    try:\n",
    "        model_bundle = load_xgb_model()\n",
    "        pred = predict_risk(model_bundle, features)\n",
    "    except Exception as exc:\n",
    "        yield (\"\", f\"Prediction failed: {exc}\", None, None)\n",
    "        return\n",
    "\n",
    "    risk_label = pred.get(\"prediction\", \"Medium\")\n",
    "    confidence = pred.get(\"probability\", 0.6)\n",
    "\n",
    "    # 7) Allocation + plots\n",
    "    split = allocation_for_risk(risk_label, goal)\n",
    "\n",
    "    # pie chart\n",
    "    fig1, ax1 = plt.subplots(figsize=(4, 3))\n",
    "    ax1.pie(split.values(), labels=split.keys(), autopct=\"%1.1f%%\", startangle=90)\n",
    "    ax1.set_title(\"Allocation\")\n",
    "\n",
    "    # 5-year projection\n",
    "    port, fd_s, gold_s = simulate_growth(amount, split, years=5)\n",
    "    yrs = np.arange(0, 6)\n",
    "    fig2, ax2 = plt.subplots(figsize=(6, 3))\n",
    "    ax2.plot(yrs, port, \"o-\", label=\"Portfolio\")\n",
    "    ax2.plot(yrs, fd_s, \"s--\", label=\"FD-only\")\n",
    "    ax2.plot(yrs, gold_s, \"d--\", label=\"Gold-only\")\n",
    "    ax2.set_title(\"5-year projection\")\n",
    "    ax2.set_xlabel(\"Year\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "    ax2.legend()\n",
    "\n",
    "    # 8) Markdown output\n",
    "    text = f\"### Risk: **{risk_label}**\\n\\nConfidence: {confidence:.2%}\\n\\nAllocation: \"\n",
    "    text += \", \".join(f\"{k}: {v*100:.0f}%\" for k, v in split.items())\n",
    "    if parse_note:\n",
    "        text = f\"**Note:** {parse_note}\\n\\n\" + text\n",
    "\n",
    "    yield (\"\", text, fig1, fig2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59f2f959-1646-44a8-9602-898194a1440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 - Stock analysis runner (filtered RAG per ticker)\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "explain_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=OPENAI_KEY,\n",
    ")\n",
    "\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"ticker\", \"trend\", \"rsi\", \"macd\"],\n",
    "    template=\"\"\"\n",
    "You're writing a short, plain-language explanation for an investor.\n",
    "\n",
    "Ticker: {ticker}\n",
    "Trend (one line): {trend}\n",
    "RSI note: {rsi}\n",
    "MACD note: {macd}\n",
    "\n",
    "Write a 2-3 sentence explanation.\n",
    "\"\"\",\n",
    ")\n",
    "explain_chain = LLMChain(llm=explain_llm, prompt=explain_prompt)\n",
    "\n",
    "\n",
    "def _filter_docs_for_ticker(docs, ticker: str):\n",
    "    \"\"\"\n",
    "    Keep only documents that look relevant to the given ticker:\n",
    "    - metadata['ticker'] == ticker (if present), OR\n",
    "    - the text itself mentions the ticker.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    t = ticker.strip().upper()\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        text = (getattr(d, \"page_content\", \"\") or \"\")\n",
    "        meta = getattr(d, \"metadata\", {}) or {}\n",
    "        meta_ticker = str(meta.get(\"ticker\", \"\")).upper()\n",
    "        if not t:\n",
    "            out.append(d)\n",
    "            continue\n",
    "\n",
    "        if meta_ticker == t or t in text.upper():\n",
    "            out.append(d)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _clean_research_snip(docs, ticker: str, max_chars: int = 1200) -> str:\n",
    "    \"\"\"\n",
    "    Build a short research snippet:\n",
    "    - Only use docs relevant to the ticker.\n",
    "    - Inside each doc, keep only sentences that mention the ticker.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"No research available.\"\n",
    "\n",
    "    t = ticker.strip().upper()\n",
    "    snippets = []\n",
    "\n",
    "    for d in docs:\n",
    "        text = (getattr(d, \"page_content\", \"\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if t:\n",
    "            # split into sentences, keep those mentioning the ticker\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "            hit_sents = [s for s in sentences if t in s.upper()]\n",
    "            text = \" \".join(hit_sents).strip()\n",
    "\n",
    "        if text:\n",
    "            snippets.append(text)\n",
    "\n",
    "    if not snippets:\n",
    "        return \"No research available.\"\n",
    "\n",
    "    joined = \"\\n\\n\".join(snippets).strip()\n",
    "    return joined[:max_chars] if len(joined) > max_chars else joined\n",
    "\n",
    "\n",
    "def run_stock_analysis_enhanced(query: str):\n",
    "    yield (\"Fetching chart...\", \"\", None)\n",
    "\n",
    "    ticker = query.strip()\n",
    "\n",
    "    # --- RAG / research text (filtered) ---\n",
    "    try:\n",
    "        client, coll = init_chroma()\n",
    "        retriever = make_retriever(4)\n",
    "        docs_raw = rag_query(ticker, retriever, coll)\n",
    "        docs_filt = _filter_docs_for_ticker(docs_raw, ticker)\n",
    "        rag_snip = _clean_research_snip(docs_filt, ticker)\n",
    "    except Exception as exc:\n",
    "        print(\"[run_stock_analysis_enhanced] RAG error:\", exc)\n",
    "        rag_snip = \"No research available.\"\n",
    "\n",
    "    # --- Price history + indicators ---\n",
    "    hist = get_history(ticker, period=\"6mo\")\n",
    "    fig = plot_with_indicators(hist, ticker) if not hist.empty else None\n",
    "\n",
    "    trend = \"insufficient data\"\n",
    "    rsi_note = \"RSI unavailable\"\n",
    "    macd_note = \"MACD unavailable\"\n",
    "\n",
    "    try:\n",
    "        if not hist.empty:\n",
    "            dfi = add_indicators(hist)\n",
    "            change = (dfi[\"Close\"].iloc[-1] - dfi[\"Close\"].iloc[0]) / (dfi[\"Close\"].iloc[0] + 1e-9)\n",
    "            trend = \"Uptrend\" if change > 0.03 else (\"Downtrend\" if change < -0.03 else \"Sideways\")\n",
    "\n",
    "            last_rsi = dfi[\"RSI14\"].iloc[-1]\n",
    "            rsi_note = (\n",
    "                f\"RSI ~{last_rsi:.1f} — \"\n",
    "                + (\"Overbought\" if last_rsi > 70 else (\"Oversold\" if last_rsi < 30 else \"Neutral\"))\n",
    "            )\n",
    "\n",
    "            m_val = dfi[\"MACD\"].iloc[-1]\n",
    "            s_val = dfi[\"MACD_Signal\"].iloc[-1]\n",
    "            macd_note = (\n",
    "                f\"MACD {m_val:.3f} vs {s_val:.3f} — \"\n",
    "                + (\"Bullish\" if m_val > s_val else \"Bearish/Neutral\")\n",
    "            )\n",
    "    except Exception as exc:\n",
    "        print(\"[run_stock_analysis_enhanced] indicator error:\", exc)\n",
    "\n",
    "    # --- Natural-language explanation ---\n",
    "    try:\n",
    "        expl = explain_chain.run(\n",
    "            {\"ticker\": ticker, \"trend\": trend, \"rsi\": rsi_note, \"macd\": macd_note}\n",
    "        )\n",
    "    except Exception:\n",
    "        expl = f\"{trend}. {rsi_note}. {macd_note}.\"\n",
    "\n",
    "    md = (\n",
    "        f\"**Research:**\\n\\n{rag_snip}\\n\\n\"\n",
    "        f\"**Chart note:**\\n\\n{expl}\\n\\n\"\n",
    "        f\"**Tech details:**\\n- Trend: {trend}\\n- {rsi_note}\\n- {macd_note}\"\n",
    "    )\n",
    "\n",
    "    yield (\"\", md, fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adb8061a-cfeb-4d8d-995e-077654d980c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 - Compare two tickers (simple & robust)\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def _download_with_ns(symbol: str, period: str = \"3y\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Try history for a symbol. If plain symbol fails and there's no suffix,\n",
    "    also try SYMBOL.NS (common for Indian listings).\n",
    "    \"\"\"\n",
    "    base = (symbol or \"\").strip()\n",
    "    if not base:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    candidates = [base]\n",
    "    if \".\" not in base:\n",
    "        candidates.append(base.upper() + \".NS\")\n",
    "\n",
    "    for sym in candidates:\n",
    "        # Try yf.download\n",
    "        try:\n",
    "            df = yf.download(sym, period=period, progress=False)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"__symbol__\"] = sym\n",
    "                return df\n",
    "        except Exception as exc:\n",
    "            print(f\"[download_with_ns] download error for {sym}: {exc}\")\n",
    "\n",
    "        # Fallback: Ticker().history\n",
    "        try:\n",
    "            df2 = yf.Ticker(sym).history(period=period)\n",
    "            if df2 is not None and not df2.empty:\n",
    "                df2[\"__symbol__\"] = sym\n",
    "                return df2\n",
    "        except Exception as exc:\n",
    "            print(f\"[download_with_ns] history error for {sym}: {exc}\")\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def run_compare_generator(sym1: str, sym2: str):\n",
    "    \"\"\"\n",
    "    Gradio generator: compares two tickers using RAG text + price data.\n",
    "\n",
    "    Returns:\n",
    "        (status_text, markdown)\n",
    "    \"\"\"\n",
    "    # First yield: status only\n",
    "    yield (\"Gathering data...\", \"\")\n",
    "\n",
    "    try:\n",
    "        # ---------- RAG text (best-effort) ----------\n",
    "        try:\n",
    "            client, coll = init_chroma()\n",
    "            retriever = make_retriever(4)\n",
    "            docs1 = rag_query(sym1, retriever, coll) or []\n",
    "            docs2 = rag_query(sym2, retriever, coll) or []\n",
    "\n",
    "            txt1 = \"\\n\".join(getattr(d, \"page_content\", \"\") for d in docs1) or \"No RAG data.\"\n",
    "            txt2 = \"\\n\".join(getattr(d, \"page_content\", \"\") for d in docs2) or \"No RAG data.\"\n",
    "        except Exception as exc:\n",
    "            print(f\"[run_compare_generator] RAG error: {exc}\")\n",
    "            txt1 = txt2 = \"No RAG available.\"\n",
    "\n",
    "        # ---------- Price history (3y, with .NS fallback) ----------\n",
    "        try:\n",
    "            h1 = _download_with_ns(sym1, period=\"3y\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[run_compare_generator] history error for {sym1}: {exc}\")\n",
    "            h1 = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            h2 = _download_with_ns(sym2, period=\"3y\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[run_compare_generator] history error for {sym2}: {exc}\")\n",
    "            h2 = pd.DataFrame()\n",
    "\n",
    "        parts = [\n",
    "            f\"## Research Summary\\n\",\n",
    "            f\"### {sym1}\\n{txt1}\\n\\n\",\n",
    "            f\"### {sym2}\\n{txt2}\\n\\n\",\n",
    "        ]\n",
    "\n",
    "        # ---------- Price-based comparison ----------\n",
    "        if (\n",
    "            h1 is not None and not h1.empty and \"Close\" in h1.columns and\n",
    "            h2 is not None and not h2.empty and \"Close\" in h2.columns\n",
    "        ):\n",
    "            try:\n",
    "                # compare_two_price_series comes from Block 2\n",
    "                cmp_md = compare_two_price_series(\n",
    "                    h1,\n",
    "                    h2,\n",
    "                    price_col=\"Close\",\n",
    "                    name1=sym1,\n",
    "                    name2=sym2,\n",
    "                )\n",
    "                parts.append(\"\\n\\n\")\n",
    "                parts.append(cmp_md)\n",
    "            except Exception as exc:\n",
    "                print(f\"[run_compare_generator] compare helper error: {exc}\")\n",
    "                parts.append(\"\\n\\n**Price data available but comparison failed.**\")\n",
    "        else:\n",
    "            parts.append(\"**Not enough price data to compare these symbols.**\")\n",
    "\n",
    "        final_md = \"\".join(parts)\n",
    "        yield (\"\", final_md)\n",
    "\n",
    "    except Exception as exc:\n",
    "        # Catch-all so Gradio never shows generic ERROR\n",
    "        yield (\"\", f\"Error while comparing {sym1} and {sym2}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2ee552e-1215-4e6b-9a92-d192867669bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 rows and 18 columns\n",
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8.7 - UI layout and launch\n",
    "import gradio as gr\n",
    "\n",
    "# try loading dataset (optional, for dropdown auto-fill)\n",
    "try:\n",
    "    df_opts = load_data()\n",
    "except Exception:\n",
    "    df_opts = None\n",
    "\n",
    "def get_choices(col, default):\n",
    "    if df_opts is None or col not in (df_opts.columns if hasattr(df_opts, \"columns\") else []):\n",
    "        return default\n",
    "    vals = sorted([str(x) for x in df_opts[col].dropna().unique().tolist()], key=str.lower)\n",
    "    return vals if vals else default\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Fixed dropdown options\n",
    "# -----------------------------\n",
    "age_opts = [\"18-25\", \"26-35\", \"36-45\", \"46-60\", \"60+\"]\n",
    "income_opts = [\"<30,000\", \"30,000-70,000\", \"70,000+\"]\n",
    "\n",
    "employment_opts = get_choices(\n",
    "    \"Employment Status\",\n",
    "    [\"Salaried\", \"Self-employed\", \"Retired\"]\n",
    ")\n",
    "\n",
    "#  Now using approved / pending / rejected\n",
    "loan_opts = [\"approved\", \"pending\", \"rejected\"]\n",
    "\n",
    "goal_opts = get_choices(\n",
    "    \"Investment Goals\",\n",
    "    [\"Growth\", \"Wealth Preservation\", \"Short-term Safety\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Gradio App\n",
    "\n",
    "with gr.Blocks(title=\"FinRagAssist\") as demo:\n",
    "    gr.Markdown(\"# **FinRagAssist — Smart Investment Advisor**\")\n",
    "\n",
    "    # RISK PROFILER TAB\n",
    "    with gr.Tab(\"Risk Profiler\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                free_text = gr.Textbox(\n",
    "                    label=\"Optional: profile text\",\n",
    "                    placeholder=\"e.g. I'm 32, salaried, 80k/month, loan approved, goal growth, invest 50k\"\n",
    "                )\n",
    "                age_in = gr.Dropdown(label=\"Age Group\", choices=age_opts, value=\"26-35\")\n",
    "                income_in = gr.Dropdown(label=\"Income Group\", choices=income_opts, value=\"30,000-70,000\")\n",
    "                emp_in = gr.Dropdown(label=\"Employment Status\", choices=employment_opts, value=employment_opts[0])\n",
    "            \n",
    "            with gr.Column():\n",
    "                loan_in = gr.Dropdown(label=\"Loan Status\", choices=loan_opts, value=\"approved\")\n",
    "                goal_in = gr.Dropdown(label=\"Investment Goal\", choices=goal_opts, value=goal_opts[0])\n",
    "                amt_in = gr.Number(label=\"Investment Amount (₹)\", value=25000)\n",
    "\n",
    "                btn = gr.Button(\"Get Recommendation\")\n",
    "\n",
    "                status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                out_md = gr.Markdown()\n",
    "                pie = gr.Plot()\n",
    "                growth = gr.Plot()\n",
    "\n",
    "        btn.click(\n",
    "            run_risk_profile,\n",
    "            inputs=[free_text, age_in, income_in, emp_in, loan_in, goal_in, amt_in],\n",
    "            outputs=[status_box, out_md, pie, growth]\n",
    "        )\n",
    "\n",
    "    # STOCK ANALYSIS TAB\n",
    "    with gr.Tab(\"Stock Analysis\"):\n",
    "        ticker = gr.Textbox(label=\"Ticker or Name\")\n",
    "        analyze_btn = gr.Button(\"Analyze\")\n",
    "\n",
    "        st_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        st_md = gr.Markdown()\n",
    "        st_fig = gr.Plot()\n",
    "\n",
    "        analyze_btn.click(\n",
    "            run_stock_analysis_enhanced,\n",
    "            inputs=[ticker],\n",
    "            outputs=[st_status, st_md, st_fig]\n",
    "        )\n",
    "\n",
    "    # COMPARE TAB\n",
    "    with gr.Tab(\"Compare Stocks\"):\n",
    "        a = gr.Textbox(label=\"Ticker 1\")\n",
    "        b = gr.Textbox(label=\"Ticker 2\")\n",
    "        cmp_btn = gr.Button(\"Compare\")\n",
    "\n",
    "        cmp_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        cmp_out = gr.Markdown()\n",
    "\n",
    "        cmp_btn.click(\n",
    "            run_compare_generator,\n",
    "            inputs=[a, b],\n",
    "            outputs=[cmp_status, cmp_out]\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb65c8-0aeb-4467-84e0-6f2059361333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32895fe-454d-41da-b8ee-df917399d28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5572ad-28b5-42d9-94f0-6c773abda849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913da680-4e41-4eb3-bc8d-7cf820efb850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e926504-a9c1-41a0-ac50-f1a2b173750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32625825-02bf-42ab-99d6-e09ac704e8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
